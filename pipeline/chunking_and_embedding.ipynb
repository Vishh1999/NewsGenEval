{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyPHBtTcWnnnxwaJjrXcAY8J"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "dd8a400c5cd2474a9437d3ef166911ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a5aecefcdd84aeaa8b7972c130fd376",
       "IPY_MODEL_feda399f0d9247df9377aa3e45560f40",
       "IPY_MODEL_6d77a5d57b754b4f9efa24540773d10e"
      ],
      "layout": "IPY_MODEL_c1cdbd4dfe3548218bb655cfb3ee588a"
     }
    },
    "9a5aecefcdd84aeaa8b7972c130fd376": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d40c62a09e74771896dc1998ac58590",
      "placeholder": "​",
      "style": "IPY_MODEL_12685baec0224a6ebae7ac8146f8f3e8",
      "value": "Batches: 100%"
     }
    },
    "feda399f0d9247df9377aa3e45560f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad238ea1770a4110b58722214ae1e6dd",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_427a8aae8ba64a30810a0c091b3fc22a",
      "value": 5
     }
    },
    "6d77a5d57b754b4f9efa24540773d10e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31d703be45934d1a82caec6860a526eb",
      "placeholder": "​",
      "style": "IPY_MODEL_dd93c2838aed44299396b06b3f9eca2f",
      "value": " 5/5 [00:02&lt;00:00,  2.64it/s]"
     }
    },
    "c1cdbd4dfe3548218bb655cfb3ee588a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d40c62a09e74771896dc1998ac58590": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12685baec0224a6ebae7ac8146f8f3e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad238ea1770a4110b58722214ae1e6dd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "427a8aae8ba64a30810a0c091b3fc22a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31d703be45934d1a82caec6860a526eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd93c2838aed44299396b06b3f9eca2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mk9WXqzOfcSG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756818263813,
     "user_tz": -60,
     "elapsed": 18264,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "11d0f283-ac11-4e82-a544-6a1dfef866dc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/67.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.8/19.8 MB\u001B[0m \u001B[31m104.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m284.2/284.2 kB\u001B[0m \u001B[31m25.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m92.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.3/103.3 kB\u001B[0m \u001B[31m9.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.5/16.5 MB\u001B[0m \u001B[31m36.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.5/72.5 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.4/105.4 kB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.6/71.6 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m510.8/510.8 kB\u001B[0m \u001B[31m35.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.7/4.7 MB\u001B[0m \u001B[31m113.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m452.2/452.2 kB\u001B[0m \u001B[31m33.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "!pip -q install nltk -U sentence-transformers chromadb tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from typing import List\n",
    "\n",
    "# Download tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set LOCAL variable values\n",
    "# Article (data) selection\n",
    "main_data = json.load(open(\"wikinews_data.json\", 'r'))\n",
    "for i, title in enumerate([d['title'] for d in main_data]):\n",
    "  print(f'{i}: {title}')\n",
    "\n",
    "# Change the index for new article\n",
    "ARTICLE_INDEX = 14\n",
    "data = main_data[ARTICLE_INDEX]\n",
    "print(f\"\\nWill be working with '{data['title']}' article data!\")\n",
    "\n",
    "source_data = data['source_data']\n",
    "chroma_collection_name = data['chroma_collection_name']\n",
    "chroma_db_path = \"chroma_db\"\n",
    "model_path = \"models\"\n",
    "print(f\"\\nStarted with the chunking and embedding of '{data['title']}' article data\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXMxnI3QkK4Q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756818285981,
     "user_tz": -60,
     "elapsed": 1684,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "fa617a01-0501-48af-9214-eb2bacdc9b85"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: US: Tulsa residents approve $814 million infrastructure package\n",
      "1: 78th British Academy Film Awards held in London\n",
      "2: Ryan Gosling cast in upcoming Star Wars film\n",
      "3: Thai officials seize 238 tons of illegal e-waste at Bangkok port\n",
      "4: 20-year-old astrophotographer captures rare solar eclipse on Saturn\n",
      "5: Scientists discover seagrass off Australia is world's largest plant\n",
      "6: India defeats New Zealand to win 2025 Champions Trophy\n",
      "7: Researchers film colossal squid in its natural habitat for the first time\n",
      "8: SpaceX will return stranded astronauts in February 2025, NASA announces\n",
      "9: Microsoft, Nware sign 10-year cloud gaming deal\n",
      "10: United Kingdom buries Queen Elizabeth II after state funeral\n",
      "11: UK heavy metal band Black Sabbath announces final performance with original lineup\n",
      "12: GSK rejects three Unilever bids to buy consumer healthcare arm, says unit was 'fundamentally undervalued'\n",
      "13: FIFA World Cup 2018 Last 16: France, Uruguay send Argentina, Portugal home\n",
      "14: European Union to reduce carbon emissions by 55% of 1990 levels by 2030\n",
      "\n",
      "Will be working with 'European Union to reduce carbon emissions by 55% of 1990 levels by 2030' article data!\n",
      "\n",
      "Started with the chunking and embedding of 'European Union to reduce carbon emissions by 55% of 1990 levels by 2030' article data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def pick_n_by_source_data_wc(word_count_source_data):\n",
    "    if word_count_source_data < 700:\n",
    "        return 4, 1\n",
    "    elif word_count_source_data < 1800:\n",
    "        return 3, 1\n",
    "    else:\n",
    "        return 2, 1\n",
    "\n",
    "\n",
    "def pair_sentences(source_data, n, stride, max_chars):\n",
    "    \"\"\"\n",
    "    Make sliding-window chunks of 'n' sentences from a single reference string.\n",
    "    Stride controls the step between windows.\n",
    "    Optionally trim each chunk to max_chars (soft cut on whitespace).\n",
    "    Returns: list of {'chunk_id', 'chunk_text'}.\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in sent_tokenize(source_data) if s.strip()]\n",
    "    chunks = []\n",
    "    if not sentences or n <= 0:\n",
    "        return chunks\n",
    "\n",
    "    for i in range(0, len(sentences) - n + 1, max(1, stride)):\n",
    "        text = \" \".join(sentences[i:i+n])\n",
    "        if max_chars and len(text) > max_chars:\n",
    "            # soft trim at last space to avoid mid-word cut\n",
    "            cut = text[:max_chars].rsplit(\" \", 1)[0]\n",
    "            text = cut if cut else text[:max_chars]\n",
    "        chunks.append({\"chunk_id\": str(i), \"chunk_text\": text})\n",
    "    return chunks\n",
    "\n",
    "\n",
    "wc_source = data['word_count_source_data']\n",
    "n, stride = pick_n_by_source_data_wc(wc_source)\n",
    "chunked_articles = pair_sentences(\n",
    "    source_data,\n",
    "    n=n,\n",
    "    stride=stride,\n",
    "    max_chars=900)\n",
    "\n",
    "print(f\"Source word count: {wc_source}\")\n",
    "print(f\"Using n={n}, stride={stride}\")\n",
    "print(f\"Total chunks: {len(chunked_articles)}\")\n",
    "if chunked_articles:\n",
    "    print(f\"Sample chunk -- {chunked_articles[0]}\")"
   ],
   "metadata": {
    "id": "uUgjsu2dtRvD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756331158080,
     "user_tz": -60,
     "elapsed": 7,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "f9ed1414-92d9-4f7a-9765-b502147259f2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Source word count: 3911\n",
      "Using n=2, stride=1\n",
      "Total chunks: 144\n",
      "Sample chunk -- {'chunk_id': '0', 'chunk_text': 'The EU has adopted ambitious new targets to curb climate change, with a pledge to make them legally binding. Under a new law agreed between member states and the EU Parliament, the bloc will cut carbon emissions by at least 55% by 2030, compared with 1990 levels.'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model name and local path\n",
    "model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "local_dir = f\"{model_path}/{model_name.split('/')[1]}\"\n",
    "\n",
    "# If model isn't already downloaded, fetch and save to local directory\n",
    "if not os.path.exists(local_dir):\n",
    "  # Download model\n",
    "  model = SentenceTransformer(model_name, device=device)\n",
    "  model.max_seq_length = 512\n",
    "  model.save(local_dir)\n",
    "  print(\"Model download complete and saved locally!\")\n",
    "else:\n",
    "  print(f\"Model already downloaded in {local_dir}. Loading from local path.\")\n",
    "  model = SentenceTransformer(local_dir, device=device)\n",
    "  model.max_seq_length = 512\n",
    "  print(f\"Loaded model from {local_dir} successfully on {device.upper()}!\")"
   ],
   "metadata": {
    "id": "c8YUQawzqayE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756818348210,
     "user_tz": -60,
     "elapsed": 62231,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "cc5a8fc4-8d08-4e56-c9e2-16eb11226271"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model already downloaded in /content/drive/MyDrive/Colab Notebooks/Dissertation/models/bge-large-en-v1.5. Loading from local path.\n",
      "Loaded model from /content/drive/MyDrive/Colab Notebooks/Dissertation/models/bge-large-en-v1.5 successfully on CUDA!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "# Example words\n",
    "words = [\"cat\", \"tiger\", \"paris\"]\n",
    "\n",
    "# Encode into embeddings\n",
    "embeddings = model.encode(words, normalize_embeddings=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarity_matrix = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Pretty print results\n",
    "df = pd.DataFrame(\n",
    "    similarity_matrix.numpy(),\n",
    "    index=words,\n",
    "    columns=words\n",
    ")\n",
    "print(df.round(3))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ou76Q2lh3Zk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756818382324,
     "user_tz": -60,
     "elapsed": 51,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "aaf15771-8f14-462c-9a04-678465a1c76d"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         cat  tiger  paris\n",
      "cat    1.000  0.792  0.654\n",
      "tiger  0.792  1.000  0.633\n",
      "paris  0.654  0.633  1.000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# This ensures GPU has enough Memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "J-t-rgzXB4qj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756818371781,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "# Define embedding + ChromaDB storage function\n",
    "def embed_and_store_chunks_in_chroma(chroma_db_path, chunked_articles,collection_name, persist_directory=\"./chroma_store\"):\n",
    "    \"\"\"\n",
    "    Embeds and stores chunked_articles in ChromaDB.\n",
    "    \"\"\"\n",
    "    documents = [f\"passage: {c['chunk_text']}\" for c in chunked_articles]\n",
    "\n",
    "    ids = [f\"{chunk['chunk_id']}\"\n",
    "           for chunk in chunked_articles]\n",
    "\n",
    "    metadatas = [{\"chunk_id\": c[\"chunk_id\"],\n",
    "                  \"orig_text\": c[\"chunk_text\"]}\n",
    "                 for c in chunked_articles]\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(documents, normalize_embeddings=True, show_progress_bar=True).tolist()\n",
    "\n",
    "    # Create a persistent path\n",
    "    db_path = chroma_db_path\n",
    "\n",
    "    # Setup Chroma client\n",
    "    chroma_client = PersistentClient(path=db_path)\n",
    "\n",
    "    # Remove existing collection with same name\n",
    "    existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "    if collection_name in existing_collections:\n",
    "      print(\"Previous collection deleted!\")\n",
    "      chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "    # Create Collection\n",
    "    collection = chroma_client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\n",
    "            \"hnsw:space\": \"cosine\",\n",
    "            \"embedder\": getattr(model, \"name_or_path\", str(model)),\n",
    "            \"doc_prefix\": \"passage: \",\n",
    "            \"normalized\": str(True),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add vectors\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "# Start the embedding and vector db storing stage\n",
    "chroma_collection = embed_and_store_chunks_in_chroma(chroma_db_path, chunked_articles, chroma_collection_name)\n",
    "print(\"Done with embedding and db storage!\")\n",
    "\n",
    "# Count check\n",
    "print(\"Total stored documents:\", chroma_collection.count())\n",
    "\n",
    "# Reload from Local (TEST)\n",
    "chroma_client = PersistentClient(path=f\"{chroma_db_path}\")\n",
    "print(\"All available collections currently stored,\")\n",
    "for col in list(chroma_client.list_collections()):\n",
    "  print(col)\n",
    "\n",
    "col = chroma_client.get_collection(name=chroma_collection_name)\n",
    "print(f\"Currently working with {col} Chroma collection!\")\n",
    "\n",
    "# View the embedding\n",
    "result = col.get(limit=1, include=[\"embeddings\"])\n",
    "embedding = result[\"embeddings\"][0]\n",
    "\n",
    "print(\"Full embedding vector length:\", len(embedding))\n",
    "print(\"First 5 values of an embedding:\", embedding[:5])   # preview"
   ],
   "metadata": {
    "id": "Eu_CiIKGqhvo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "dd8a400c5cd2474a9437d3ef166911ca",
      "9a5aecefcdd84aeaa8b7972c130fd376",
      "feda399f0d9247df9377aa3e45560f40",
      "6d77a5d57b754b4f9efa24540773d10e",
      "c1cdbd4dfe3548218bb655cfb3ee588a",
      "5d40c62a09e74771896dc1998ac58590",
      "12685baec0224a6ebae7ac8146f8f3e8",
      "ad238ea1770a4110b58722214ae1e6dd",
      "427a8aae8ba64a30810a0c091b3fc22a",
      "31d703be45934d1a82caec6860a526eb",
      "dd93c2838aed44299396b06b3f9eca2f"
     ]
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756331173317,
     "user_tz": -60,
     "elapsed": 4620,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "4b6d9a8e-4bf0-4eee-df28-1fabb7003c94"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd8a400c5cd2474a9437d3ef166911ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Previous collection deleted!\n",
      "Done with embedding and db storage!\n",
      "Total stored documents: 144\n",
      "All available collections currently stored,\n",
      "Collection(name=gsk_vector_embedding_store)\n",
      "Collection(name=CT_vector_embedding_store)\n",
      "Collection(name=colossal_squid_vector_embedding_store)\n",
      "Collection(name=ryan_gosling_vector_embedding_store)\n",
      "Collection(name=fifa_wc_vector_embedding_store)\n",
      "Collection(name=tulsa_residents_vector_embedding_store)\n",
      "Collection(name=eu_reduce_vector_embedding_store)\n",
      "Collection(name=spacex_crew_vector_embedding_store)\n",
      "Collection(name=Queen_funeral_vector_embedding_store)\n",
      "Collection(name=microsoft_vector_embedding_store)\n",
      "Collection(name=queen_funeral_vector_embedding_store)\n",
      "Collection(name=black_sabbath_vector_embedding_store)\n",
      "Collection(name=bafta_vector_embedding_store)\n",
      "Collection(name=ewaste_vector_embedding_store)\n",
      "Collection(name=saturn_vector_embedding_store)\n",
      "Collection(name=seagrass_vector_embedding_store)\n",
      "Currently working with Collection(name=eu_reduce_vector_embedding_store) Chroma collection!\n",
      "Full embedding vector length: 1024\n",
      "First 5 values of an embedding: [ 0.01028623  0.00735129  0.00646743  0.04258419 -0.03631675]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _get_first_hit(results, keywords):\n",
    "    \"\"\"\n",
    "    Finds the rank and max similarity score of the first document with a keyword hit.\n",
    "    Searches both document content and metadata.\n",
    "    \"\"\"\n",
    "    first_hit_rank = None\n",
    "    max_similarity = 0\n",
    "\n",
    "    docs = results[\"documents\"][0]\n",
    "    distances = results['distances'][0]\n",
    "    metadatas = results.get('metadatas', [None])[0]\n",
    "\n",
    "    for rank, (doc, dist) in enumerate(zip(docs, distances), start=1):\n",
    "        # Check for keyword in document content\n",
    "        hit_in_doc = any(k.lower() in doc.lower() for k in keywords)\n",
    "\n",
    "        # Check for keyword in metadata, converting to string\n",
    "        hit_in_metadata = False\n",
    "        if metadatas:\n",
    "            hit_in_metadata = any(k.lower() in str(metadatas[rank - 1]).lower() for k in keywords)\n",
    "\n",
    "        similarity = 1 - dist\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "\n",
    "        if (hit_in_doc or hit_in_metadata) and first_hit_rank is None:\n",
    "            first_hit_rank = rank\n",
    "\n",
    "    return first_hit_rank, max_similarity\n",
    "\n",
    "\n",
    "def evaluate_suite(test_data, model, chroma_collection, k=10):\n",
    "    \"\"\"\n",
    "    Evaluates a full test data suite of queries & keywords\n",
    "\n",
    "    Returns\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for suite_idx, suite in enumerate(test_data):\n",
    "        for q in suite['queries']:\n",
    "            # Embedding query\n",
    "            query_embedding = model.encode(\"query: \" + q, normalize_embeddings=True)\n",
    "\n",
    "            # Get top-k candidates\n",
    "            results = chroma_collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=k,\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "\n",
    "            # Compute metrics for this query\n",
    "            found_keyword = None\n",
    "            for kw in suite['keywords']:\n",
    "                for doc in results[\"documents\"][0]:\n",
    "                    if kw.lower() in doc.lower():\n",
    "                        found_keyword = kw\n",
    "                        break\n",
    "                if not found_keyword and results.get(\"metadatas\"):\n",
    "                    for meta in results[\"metadatas\"][0]:\n",
    "                        if kw.lower() in str(meta).lower():\n",
    "                            found_keyword = kw\n",
    "                            break\n",
    "                if found_keyword:\n",
    "                    break\n",
    "\n",
    "            keyword_present = found_keyword is not None\n",
    "            keyword = found_keyword if found_keyword else None\n",
    "            first_hit_rank, max_similarity = _get_first_hit(results, suite['keywords'])\n",
    "            mrr_at_k = (1.0 / first_hit_rank) if first_hit_rank else 0.0\n",
    "            recall_at_k = 1 if first_hit_rank else 0\n",
    "\n",
    "            all_results.append({\n",
    "                'suite': suite_idx,\n",
    "                'query': q,\n",
    "                'keyword_present': keyword_present,\n",
    "                'keyword': keyword,\n",
    "                f'first_hit_rank@{k}': first_hit_rank,\n",
    "                f'max_similarity_score@{k}': round(max_similarity, 4),\n",
    "                f'Recall@{k}': recall_at_k,\n",
    "                f'MRR@{k}': mrr_at_k\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Summary\n",
    "    total_queries = len(df)\n",
    "    hit_count = df[f'first_hit_rank@{k}'].notna().sum()\n",
    "    hit_rate = hit_count / total_queries if total_queries > 0 else 0\n",
    "    mean_mrr = df[f'MRR@{k}'].mean()\n",
    "    mean_similarity = df[f'max_similarity_score@{k}'].mean()\n",
    "\n",
    "    print(f\"Total Queries: {total_queries}\")\n",
    "    print(f\"Queries with a Hit: {hit_count}\")\n",
    "    print(f\"Hit Rate (Recall@{k}): {hit_rate:.2%}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mean_mrr:.3f}\")\n",
    "    print(f\"Mean Max Similarity Score: {mean_similarity:.4f}\")\n",
    "\n",
    "    summary_df = pd.DataFrame([{'Total Queries': total_queries,\n",
    "                               \"Queries with a Hit\": hit_count,\n",
    "                               f\"Hit Rate (Recall@{k})\": hit_rate,\n",
    "                               \"Mean Reciprocal Rank (MRR)\": mean_mrr,\n",
    "                               \"Mean Max Similarity Score\": mean_similarity}])\n",
    "\n",
    "    # Misses\n",
    "    misses = df[df[f'first_hit_rank@{k}'].isna()]\n",
    "    if not misses.empty:\n",
    "        print(f\"\\nQueries with No Hit in Top {k}:\")\n",
    "        for _, row in misses.iterrows():\n",
    "            print(f\"  Suite {row['suite']}: {row['query']}\")\n",
    "    else:\n",
    "        print(f\"\\nNo queries missed the top {k} results!\")\n",
    "\n",
    "    return df, summary_df\n",
    "\n",
    "\n",
    "# load test data from data dictionary\n",
    "test_data = data['embedding_test_data_suite']\n",
    "\n",
    "# top embeddings to search for\n",
    "k_value = 5\n",
    "\n",
    "results_dense, summary = evaluate_suite(test_data, model, chroma_collection, k=k_value)\n",
    "results_dense.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{data['title'].replace(' ', '_')}_embedding_metric_scores_full.csv\", index=False)\n",
    "summary.to_csv(f\"/content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{data['title'].replace(' ', '_')}_embedding_metric_scores_summary.csv\", index=False)\n",
    "\n",
    "data['embedding_results'] = results_dense.to_dict('records')\n",
    "main_data[ARTICLE_INDEX] = data\n",
    "json.dump(main_data, open(\"wikinews_data.json\", \"w\"), indent=3, force_ascii=False, orient=\"records\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gt2rJkPTMlFw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756331175452,
     "user_tz": -60,
     "elapsed": 2126,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "71cc2178-5413-45f2-ef54-8794b3d42621"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Queries: 95\n",
      "Queries with a Hit: 90\n",
      "Hit Rate (Recall@5): 94.74%\n",
      "Mean Reciprocal Rank (MRR): 0.834\n",
      "Mean Max Similarity Score: 0.7393\n",
      "\n",
      "Queries with No Hit in Top 5:\n",
      "  Suite 2: EU's net zero emissions target date?\n",
      "  Suite 2: EU's long-term climate goal?\n",
      "  Suite 11: Environmental carbon capture methods?\n",
      "  Suite 16: Consumer changes needed for net zero?\n",
      "  Suite 16: What must people do differently for climate targets?\n"
     ]
    }
   ]
  }
 ]
}
