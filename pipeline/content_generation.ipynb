{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyPfMBnk+2USjbsch9p6FzvP"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -q -U sentence-transformers accelerate transformers bitsandbytes \\\n",
    "    --prefer-binary --extra-index-url https://download.pytorch.org/whl/cu118 chromadb\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "collapsed": true,
    "id": "zaLC8PM2l7v3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756338838993,
     "user_tz": -60,
     "elapsed": 49507,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "ed90a11c-f6cd-4c27-840a-67d33bc2282f"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/67.3 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/67.3 kB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m61.4/67.3 kB\u001B[0m \u001B[31m873.7 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m835.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.3/61.3 MB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.8/19.8 MB\u001B[0m \u001B[31m48.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m284.2/284.2 kB\u001B[0m \u001B[31m26.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m90.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m103.3/103.3 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.5/16.5 MB\u001B[0m \u001B[31m50.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.5/72.5 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.4/105.4 kB\u001B[0m \u001B[31m11.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.6/71.6 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m510.8/510.8 kB\u001B[0m \u001B[31m38.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.7/4.7 MB\u001B[0m \u001B[31m61.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m452.2/452.2 kB\u001B[0m \u001B[31m32.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for pypika (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFnTOtFObiQ9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343523387,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "9078b493-38f7-4f3d-8042-3a69f2716fe9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0: US: Tulsa residents approve $814 million infrastructure package\n",
      "1: 78th British Academy Film Awards held in London\n",
      "2: Ryan Gosling cast in upcoming Star Wars film\n",
      "3: Thai officials seize 238 tons of illegal e-waste at Bangkok port\n",
      "4: 20-year-old astrophotographer captures rare solar eclipse on Saturn\n",
      "5: Scientists discover seagrass off Australia is world's largest plant\n",
      "6: India defeats New Zealand to win 2025 Champions Trophy\n",
      "7: Researchers film colossal squid in its natural habitat for the first time\n",
      "8: SpaceX will return stranded astronauts in February 2025, NASA announces\n",
      "9: Microsoft, Nware sign 10-year cloud gaming deal\n",
      "10: United Kingdom buries Queen Elizabeth II after state funeral\n",
      "11: UK heavy metal band Black Sabbath announces final performance with original lineup\n",
      "12: GSK rejects three Unilever bids to buy consumer healthcare arm, says unit was 'fundamentally undervalued'\n",
      "13: FIFA World Cup 2018 Last 16: France, Uruguay send Argentina, Portugal home\n",
      "14: European Union to reduce carbon emissions by 55% of 1990 levels by 2030\n",
      "\n",
      "Will be working with 'European Union to reduce carbon emissions by 55% of 1990 levels by 2030' article data!\n",
      "\n",
      "Started with the plain LLM & RAG-enhanced LLM generation of 'European Union to reduce carbon emissions by 55% of 1990 levels by 2030' article\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Set LOCAL variable values\n",
    "# Article (data) selection\n",
    "main_data = json.load(open(\"wikinews_data.json\", 'r'))\n",
    "for i, title in enumerate([d['title'] for d in main_data]):\n",
    "  print(f'{i}: {title}')\n",
    "\n",
    "# Change the index for new article\n",
    "ARTICLE_INDEX = 14\n",
    "data = main_data[ARTICLE_INDEX]\n",
    "print(f\"\\nWill be working with '{data['title']}' article data!\")\n",
    "\n",
    "source_data = data['source_data']\n",
    "chroma_collection_name = data['chroma_collection_name']\n",
    "chroma_db_path = \"chroma_db\"\n",
    "model_path = \"models\"\n",
    "print(f\"\\nStarted with the plain LLM & RAG-enhanced LLM generation of '{data['title']}' article\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face login\n",
    "token = open(\"hf_token.txt\", \"r\").read().strip()\n",
    "login(token=token, add_to_git_credential=True)\n",
    "print(\"Logged in to Hugging Face and the access token is cached!\")"
   ],
   "metadata": {
    "id": "vry9heN6jluc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756338873254,
     "user_tz": -60,
     "elapsed": 1826,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "4a6a9561-647f-4e33-e63d-d66f2bae8da0"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Logged in to Hugging Face and the access token is cached!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch, os\n",
    "\n",
    "# Load LLM\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model name and local path\n",
    "llm_name = \"mistral-7b-instruct\"\n",
    "local_dir = f\"{model_path}/{llm_name}\"\n",
    "\n",
    "# If model isn't already downloaded, fetch and save to local directory\n",
    "if not os.path.exists(local_dir):\n",
    "    print(f\"Downloading {llm_name} to {local_dir} ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer.save_pretrained(local_dir)\n",
    "    model.save_pretrained(local_dir)\n",
    "    print(\"Model download complete and saved locally!\")\n",
    "else:\n",
    "    print(f\"Model already downloaded in {local_dir}. Loading from local path.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_dir,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Loaded model from {local_dir} successfully on {device.upper()}!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1sqimyg3h4GG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756342268701,
     "user_tz": -60,
     "elapsed": 4042,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "8b9ad7fe-151b-4e92-d78a-44dfa33bf62d"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model already downloaded in /content/drive/MyDrive/Colab Notebooks/Dissertation/models/mistral-7b-instruct. Loading from local path.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded model from /content/drive/MyDrive/Colab Notebooks/Dissertation/models/mistral-7b-instruct successfully on CUDA!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "UChNBZPlvaYJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343540198,
     "user_tz": -60,
     "elapsed": 8,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    }
   },
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import textwrap\n",
    "import re\n",
    "\n",
    "# Count number of words and tokens in the original Human article\n",
    "article_text = data['article_text']\n",
    "\n",
    "print(\"HUMAN ARTICLE -- \")\n",
    "print(data['title'])\n",
    "wrapper = lambda text: textwrap.fill(text, width=100)\n",
    "print(wrapper(article_text))\n",
    "\n",
    "human_article_words_list = re.findall(r\"\\b\\w+\\b\", article_text)\n",
    "print(f\"Word Count: {len(human_article_words_list)}\")\n",
    "\n",
    "human_article_tokens_list = tokenizer.encode(article_text, truncation=False, add_special_tokens=False)\n",
    "print(f\"Token Count: {len(human_article_tokens_list)}\")"
   ],
   "metadata": {
    "id": "nlXuFI0xpUeC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343541819,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "40383bf5-6be1-4355-a38d-1d2aa7b22159"
   },
   "execution_count": 89,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HUMAN ARTICLE -- \n",
      "European Union to reduce carbon emissions by 55% of 1990 levels by 2030\n",
      "On Wednesday, the European Union (EU) set a new goal to cut carbon emissions by at least 55% from\n",
      "1990 levels by 2030 and reach zero net emissions by 2050, increasing the reduction target set in\n",
      "2014, which was 40% by 2030 and 80-95% by 2050.The EU Climate Law, agreed after 14 hours of\n",
      "negotiations includes the establishment of a 15-member independent body to advise on proposed\n",
      "climate policies and increased investment in 'carbon sinks'. According to members of the European\n",
      "Parliament (MEP) those will ensure 57% net reduction target is in place. This claim has been\n",
      "disputed by senior policy officer for climate and energy at the European Environmental Bureau\n",
      "Barbara Mariani, who says the measures equal a 52.8% cut in actual emissions.A September 2020 press\n",
      "release by the European Commission (EC) promised 'following broad public consultation and thorough\n",
      "impact assessments, the Commission will come forward with the corresponding legislative proposals by\n",
      "June 2021', including, according to the BBC News a limit on CO2 removal that counts toward the\n",
      "target, encouraging member states to actually reduce carbon emissions instead of merely removing\n",
      "them. 'Adopting the new target in time would allow the EU to communicate its higher ambition to\n",
      "international partners well ahead of the 2021 UN Climate Conference (COP26) in Glasgow and set the\n",
      "bar for others to follow', the press release read.This decision means the EU is to spend at least\n",
      "30% of its €1.8 trillion long-term budget on the concerns related to the climate. Poland has adopted\n",
      "its own climate strategy. According to Reuters, Poland is the only EU member state to refuse a\n",
      "pledge to climate neutrality, instead aiming to reduce emissions by 30% by 2030. The country relies\n",
      "on coal for 80% of its electricity, but will move towards renewable energy and its first nuclear\n",
      "power plant, Reuters reported.The EC president Ursula von der Leyen said the decision leads 'the EU\n",
      "on a green path for a generation'. Criticising this decision, MEP Michael Bloss said, '[w]e fought\n",
      "hard but achieved little', as well as saying the law was a 'big disappointment.' Director of Climate\n",
      "Action Network Europe Wendel Trio said '[t]he 'at least 55% emission reduction target for 2030' is\n",
      "not in line with the Paris Agreement's ambition to limit temperature rise to 1.5C by the end of the\n",
      "century', adding the law was 'rushed' and 'not the kind of climate law that will help the EU to lead\n",
      "the global efforts to tackle climate change'.MEP and chair of the European Parliament's\n",
      "environmental committee Pascal Canfin said 'it was not possible' to convince the member states 'to\n",
      "change the wording 'at least 55 net'', and while 'parliament was obviously ready to go even\n",
      "further', he said 'the compromise found is ambitious: we are going to do two and a half times more\n",
      "in nine years than what we have done in the last 10 years in Europe.'According to The Guardian under\n",
      "current measures the EU is expected to reduce emissions by 46% by 2030. The EC's press release said\n",
      "between 1990 and 2019, green house gas emissions fell by a quarter.Per multiple reports, the United\n",
      "Kingdom set its target to 78% by 2035.\n",
      "Word Count: 547\n",
      "Token Count: 777\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def estimate_context_budget(word_bin):\n",
    "    # Bin ceilings\n",
    "    if word_bin == \"Short\":\n",
    "        cap = 1400\n",
    "    elif word_bin == \"Medium\":\n",
    "        cap = 2200\n",
    "    else:\n",
    "        cap = 3000\n",
    "\n",
    "    return cap\n",
    "\n",
    "\n",
    "reference_chunks = [s.strip() for s in sent_tokenize(source_data) if s.strip()]\n",
    "\n",
    "# de-dupe while preserving order\n",
    "seen = set()\n",
    "reference_chunks = [c for c in reference_chunks if not (c in seen or seen.add(c))]\n",
    "\n",
    "MAX_CONTEXT_TOKEN_LIMIT = estimate_context_budget(data['word_count_bin'])\n",
    "\n",
    "sampled, tok_count = [], 0\n",
    "\n",
    "# First n approach\n",
    "for chunk in reference_chunks:\n",
    "    tlen = len(tokenizer.encode(chunk, add_special_tokens=False))\n",
    "    if tok_count + tlen > MAX_CONTEXT_TOKEN_LIMIT:\n",
    "        break\n",
    "    sampled.append(chunk)\n",
    "    tok_count += tlen\n",
    "\n",
    "context_text = \"\\n\".join(sampled)\n",
    "context_tokens = tokenizer.encode(\n",
    "    context_text,\n",
    "    truncation=True,\n",
    "    max_length=MAX_CONTEXT_TOKEN_LIMIT,\n",
    "    add_special_tokens=False\n",
    "    )\n",
    "print(f\"Token Count (Plain LLM context): {len(context_tokens)}\")\n",
    "context_text = tokenizer.decode(context_tokens)\n",
    "\n",
    "print(f\"Sample context Text: {context_text[:100]}\")"
   ],
   "metadata": {
    "id": "CMjZNI_2DPIF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343547762,
     "user_tz": -60,
     "elapsed": 26,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "2778637e-2800-42b0-b8c7-e0878be61bf3"
   },
   "execution_count": 90,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Token Count (Plain LLM context): 3000\n",
      "Sample context Text: The EU has adopted ambitious new targets to curb climate change, with a pledge to make them legally \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt Design\n",
    "prompt = f\"\"\"\n",
    "You are a Wikinews journalist. Based ONLY on the CONTEXT below, write a complete, high-quality news article.\n",
    "\n",
    "### CONTEXT\n",
    "{context_text}\n",
    "\n",
    "### ARTICLE RULES\n",
    "- The article's body must be between {len(human_article_words_list) + 1} and {int(len(human_article_words_list) * 1.10)} words long.\n",
    "- If you’re close to the upper bound, finish the current sentence and stop.\n",
    "- End with ONE clear concluding sentence, then on a new line output exactly: <END>\n",
    "- Do not output anything after <END>.\n",
    "- Start with the Article Title: (Use exactly as given) {data['title']}.\n",
    "- Always use past tense for writing the article.\n",
    "- Write a lead summary: a short paragraph capturing who, what, when, where.\n",
    "- Follow it with 2–3 paragraphs with details, using the CONTEXT only.\n",
    "- Use the inverted pyramid style: most important and newsworthy facts first, less immediate background details later.\n",
    "- Use neutral tone no opinions, no bullet points and no Q&A format.\n",
    "- Do not make up anything not supported by the CONTEXT.\n",
    "- Do not repeat the same fact; if similar sentences appear in the CONTEXT, consolidate them once.\n",
    "- Do not add headings or extra sections.\n",
    "- Ensure spelling and grammar are correct and consistent.\n",
    "- Use professional journalistic English.\n",
    "- Ignore any bracketed meta lines like [If you like this prompt...] and do NOT include them in the article.\n",
    "- Do not include any disclaimers, model metadata, license text, or statements like 'This article was generated'. Output only the article text.\n",
    "\n",
    "[Start the article below]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize with padding and attention mask\n",
    "input_data = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(model.device)\n",
    "\n",
    "# Set pad_token_id\n",
    "pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "# Generate\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_data[\"input_ids\"],\n",
    "    attention_mask=input_data[\"attention_mask\"],\n",
    "    # min_new_tokens = int(len(human_article_tokens_list) * 1.02),\n",
    "    # max_new_tokens = int(len(human_article_tokens_list) * 1.20)\n",
    "    max_new_tokens= int(len(human_article_words_list) * 1.45) + 70,\n",
    "    do_sample=False,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    # num_beams=4,\n",
    "    # early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode\n",
    "plain_llm_version = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove the prompt and the end of article placeholders from the LLM response\n",
    "for unwanted_string in [prompt, '[End of article]', '<END_ARTICLE>', '<END>']:\n",
    "  plain_llm_version = plain_llm_version.replace(unwanted_string, \"\")\n",
    "\n",
    "print(\"PLAIN LLM ARTICLE -- \")\n",
    "print(wrapper(plain_llm_version))\n",
    "words = plain_llm_version.split()\n",
    "print(f\"Word Count: {len(words)}\")\n",
    "tokens = tokenizer.encode(plain_llm_version, truncation=False, add_special_tokens=False)\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# Save\n",
    "filename = f\"{data['title']}_plain_LLM_article_V3.txt\".replace(\" \", \"_\")\n",
    "with open(f\"/content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(plain_llm_version)\n",
    "data['plain_llm_article'] = plain_llm_version\n",
    "print(f\"Plain LLM article saved to /content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{filename}!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc8lJl0xpvEw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343675362,
     "user_tz": -60,
     "elapsed": 121931,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "outputId": "d22a40bf-c0bb-431c-d4f6-edc514f8ca44"
   },
   "execution_count": 91,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PLAIN LLM ARTICLE -- \n",
      " European Union to reduce carbon emissions by 55% of 1990 levels by 2030  The European Union (EU)\n",
      "has agreed on ambitious new targets to curb climate change, with a pledge to make them legally\n",
      "binding. Under a new law agreed between member states and the EU Parliament, the bloc will cut\n",
      "carbon emissions by at least 55% by 2030, compared with 1990 levels.  The EU Parliament had pushed\n",
      "for a higher target of a 60% reduction. EU Commission chief Ursula von der Leyen said, \"Our\n",
      "political commitment to becoming the first climate neutral continent by 2050 is now also a legal\n",
      "one.\" The deal comes ahead of a virtual summit of world leaders later this week, where the US is\n",
      "expected to announce its own climate targets for 2030.  US President Joe Biden, who will lead the\n",
      "meeting, rejoined the Paris climate agreement in his first day in office and has previously\n",
      "committed to reaching net zero emissions by 2050. The UK, meanwhile, announced radical plans to cut\n",
      "carbon emissions by 78% by 2035 earlier this week, although environmentalists warn that the\n",
      "government has consistently failed to achieve previous targets set by its independent Climate Change\n",
      "Committee (CCC).  The EU Climate Law was agreed in the early hours of Wednesday after months of\n",
      "talks. It sets a limit on the levels of CO2 removal that can count towards the 2030 target, to\n",
      "ensure that states actively lower emissions rather than removing them from the atmosphere through\n",
      "forests, for example. A 15-member independent council will also be established to advise the EU on\n",
      "climate measures and targets. The EU Commission will announce a package of climate laws in June to\n",
      "support the plans.  The target to reduce emissions by 55% by 2030 was initially announced by EU\n",
      "leaders in December but there had been pressure from the EU Parliament and environmental groups for\n",
      "the law to go even further. Previous EU targets had called for a 40% cut.  However, environmental\n",
      "groups were quick to criticize the deal. Wendel Trio, the director of Climate Action Network (CAN)\n",
      "Europe, an alliance of environmental NGOs, said, \"The 'at least 55% emission reduction target for\n",
      "2030' is not in line with the Paris agreement's ambition to limit temperature rise to 1.5C by the\n",
      "end of the century.\"  The agreement fails to tie up all member states to zero emissions by 2050 as a\n",
      "consequence of Poland opting out of a provisional leaders' agreement on the target last December.\n",
      "Poland relies on coal for 80% of the country's electricity. MEPs, who had sought a 60% cut by 2030,\n",
      "also failed to significantly raise the ambitions of the EU's capitals despite the claims on\n",
      "Wednesday that the bloc was showing global leadership on the climate emergency.\n",
      "Word Count: 459\n",
      "Token Count: 649\n",
      "Plain LLM article saved to /content/drive/MyDrive/Colab Notebooks/Dissertation/Article_14/European_Union_to_reduce_carbon_emissions_by_55%_of_1990_levels_by_2030_plain_LLM_article_V3.txt!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model_path = f\"{model_path}/bge-large-en-v1.5\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embedding_model = SentenceTransformer(embedding_model_path, device=device)\n",
    "embedding_model.max_seq_length = 512"
   ],
   "metadata": {
    "id": "7qwFm90_-RH_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756340193167,
     "user_tz": -60,
     "elapsed": 1307,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "# Load ChromaDB collection\n",
    "client = PersistentClient(path=chroma_db_path)\n",
    "collection = client.get_collection(name=chroma_collection_name)\n",
    "\n",
    "# Choose candidate pool by bin\n",
    "if data['word_count_bin'] == \"Long\":\n",
    "    N_RESULTS = 120\n",
    "elif data['word_count_bin'] == \"Medium\":\n",
    "    N_RESULTS = 100\n",
    "else:\n",
    "    N_RESULTS = 70\n",
    "\n",
    "# Encode first query (title)\n",
    "query = data['title']\n",
    "q1 = embedding_model.encode(\"query: \" + query, normalize_embeddings=True)\n",
    "\n",
    "# Encode second query (title + \"key facts\")\n",
    "q2 = embedding_model.encode(\"query: \" + (query + \" key facts and details\"), normalize_embeddings=True)\n",
    "\n",
    "# q1, q2 Queries for Retrieval\n",
    "res1 = collection.query(query_embeddings=[q1.tolist()], n_results=N_RESULTS, include=[\"documents\", \"distances\"])\n",
    "res2 = collection.query(query_embeddings=[q2.tolist()], n_results=N_RESULTS, include=[\"documents\", \"distances\"])\n",
    "\n",
    "# Retrieve top-N documents for both queries\n",
    "res1 = collection.query(query_embeddings=[q1.tolist()], n_results=50, include=[\"documents\", \"distances\"])\n",
    "res2 = collection.query(query_embeddings=[q2.tolist()], n_results=50, include=[\"documents\", \"distances\"])\n",
    "\n",
    "ids1 = res1.get(\"ids\", [[]])[0]\n",
    "ids2 = res2.get(\"ids\", [[]])[0]\n",
    "\n",
    "docs1, dists1 = res1[\"documents\"][0], res1[\"distances\"][0]\n",
    "docs2, dists2 = res2[\"documents\"][0], res2[\"distances\"][0]\n",
    "\n",
    "# Merge: keep highest-sim per id\n",
    "merged = {}\n",
    "for _id, doc, dist in zip(ids1, docs1, dists1):\n",
    "    sim = 1 - dist\n",
    "    if (_id not in merged) or (sim > merged[_id][\"sim\"]):\n",
    "        merged[_id] = {\"doc\": doc, \"sim\": sim}\n",
    "\n",
    "for _id, doc, dist in zip(ids2, docs2, dists2):\n",
    "    sim = 1 - dist\n",
    "    if (_id not in merged) or (sim > merged[_id][\"sim\"]):\n",
    "        merged[_id] = {\"doc\": doc, \"sim\": sim}\n",
    "\n",
    "# Sort and select unique top-N\n",
    "pairs = sorted(((id_, v[\"sim\"], v[\"doc\"]) for id_, v in merged.items()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Build up to the SAME adaptive budget as plain LLM + 100 tokens to compensate for passage: in ach and every embedding\n",
    "MAX_CONTEXT_TOKEN_LIMIT = estimate_context_budget(data['word_count_bin']) + 100\n",
    "\n",
    "sampled_docs, tok_count = [], 0\n",
    "for _, _, doc in pairs:\n",
    "    tlen = len(tokenizer.encode(doc, add_special_tokens=False))\n",
    "    if tok_count + tlen > MAX_CONTEXT_TOKEN_LIMIT:\n",
    "        break\n",
    "    sampled_docs.append(doc)\n",
    "    tok_count += tlen\n",
    "\n",
    "context_text = \"\\n\".join(sampled_docs)\n",
    "\n",
    "# Print with ids and similarities\n",
    "print(\"Selected top unique context chunks:\")\n",
    "for rank, (id_, sim, doc) in enumerate(pairs):\n",
    "    print(f\"{rank}. id={id_}, similarity={sim}\")\n",
    "\n",
    "# Truncate to stay within model token limit\n",
    "context_tokens = tokenizer.encode(\n",
    "    context_text,\n",
    "    truncation=True,\n",
    "    max_length=MAX_CONTEXT_TOKEN_LIMIT,\n",
    "    add_special_tokens=False\n",
    "    )\n",
    "\n",
    "print(f\"Token Count (RAG context): {len(context_tokens)}\")\n",
    "context_text = tokenizer.decode(context_tokens)"
   ],
   "metadata": {
    "id": "azLRE-i77hMk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343678225,
     "user_tz": -60,
     "elapsed": 2860,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dd7da699-a93c-4abd-9786-f72b240cc10f"
   },
   "execution_count": 92,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected top unique context chunks:\n",
      "0. id=1, similarity=0.8093758821487427\n",
      "1. id=12, similarity=0.8036229014396667\n",
      "2. id=21, similarity=0.792255699634552\n",
      "3. id=0, similarity=0.7887791395187378\n",
      "4. id=22, similarity=0.7875897288322449\n",
      "5. id=30, similarity=0.7846798300743103\n",
      "6. id=89, similarity=0.7845910787582397\n",
      "7. id=114, similarity=0.7830570340156555\n",
      "8. id=16, similarity=0.7824594974517822\n",
      "9. id=35, similarity=0.7815335392951965\n",
      "10. id=143, similarity=0.7783237099647522\n",
      "11. id=36, similarity=0.7778615355491638\n",
      "12. id=29, similarity=0.7770565152168274\n",
      "13. id=11, similarity=0.7770453691482544\n",
      "14. id=115, similarity=0.7738066911697388\n",
      "15. id=34, similarity=0.7728836536407471\n",
      "16. id=112, similarity=0.7728740572929382\n",
      "17. id=98, similarity=0.7713354229927063\n",
      "18. id=20, similarity=0.7675957679748535\n",
      "19. id=19, similarity=0.7674711346626282\n",
      "20. id=23, similarity=0.766655445098877\n",
      "21. id=7, similarity=0.7665737271308899\n",
      "22. id=107, similarity=0.7651887536048889\n",
      "23. id=88, similarity=0.764456033706665\n",
      "24. id=113, similarity=0.7602684497833252\n",
      "25. id=43, similarity=0.7595125436782837\n",
      "26. id=9, similarity=0.7555343508720398\n",
      "27. id=111, similarity=0.7547959089279175\n",
      "28. id=15, similarity=0.7544511556625366\n",
      "29. id=8, similarity=0.7539962530136108\n",
      "30. id=2, similarity=0.753811240196228\n",
      "31. id=28, similarity=0.7518466711044312\n",
      "32. id=84, similarity=0.7487003803253174\n",
      "33. id=83, similarity=0.7473724484443665\n",
      "34. id=42, similarity=0.7450928688049316\n",
      "35. id=18, similarity=0.7437787652015686\n",
      "36. id=6, similarity=0.7434820532798767\n",
      "37. id=117, similarity=0.7429579496383667\n",
      "38. id=39, similarity=0.742874264717102\n",
      "39. id=99, similarity=0.7402584552764893\n",
      "40. id=92, similarity=0.7396156787872314\n",
      "41. id=141, similarity=0.738706648349762\n",
      "42. id=93, similarity=0.736202597618103\n",
      "43. id=48, similarity=0.7348311543464661\n",
      "44. id=94, similarity=0.7335344552993774\n",
      "45. id=10, similarity=0.7334139347076416\n",
      "46. id=91, similarity=0.7329555749893188\n",
      "47. id=90, similarity=0.728818416595459\n",
      "48. id=61, similarity=0.7279406785964966\n",
      "49. id=4, similarity=0.7266290783882141\n",
      "50. id=134, similarity=0.7135239243507385\n",
      "51. id=82, similarity=0.7108081579208374\n",
      "52. id=142, similarity=0.7101045846939087\n",
      "Token Count (RAG context): 3100\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt Design\n",
    "prompt = f\"\"\"\n",
    "You are a Wikinews journalist. Based ONLY on the CONTEXT below, write a complete, high-quality news article.\n",
    "\n",
    "### CONTEXT\n",
    "{context_text}\n",
    "\n",
    "### ARTICLE RULES\n",
    "- The article's body must be between {len(human_article_words_list) + 1} and {int(len(human_article_words_list) * 1.10)} words long.\n",
    "- If you’re close to the upper bound, finish the current sentence and stop.\n",
    "- End with ONE clear concluding sentence, then on a new line output exactly: <END>\n",
    "- Do not output anything after <END>.\n",
    "- Start with the Article Title: (Use exactly as given) {data['title']}.\n",
    "- Always use past tense for writing the article.\n",
    "- Write a lead summary: a short paragraph capturing who, what, when, where.\n",
    "- Follow it with 2–3 paragraphs with details, using the CONTEXT only.\n",
    "- Use the inverted pyramid style: most important and newsworthy facts first, less immediate background details later.\n",
    "- Use neutral tone no opinions, no bullet points and no Q&A format.\n",
    "- Do not make up anything not supported by the CONTEXT.\n",
    "- Do not repeat the same fact; if similar sentences appear in the CONTEXT, consolidate them once.\n",
    "- Do not add headings or extra sections.\n",
    "- Ensure spelling and grammar are correct and consistent.\n",
    "- Use professional journalistic English.\n",
    "- Ignore any bracketed meta lines like [If you like this prompt...] and do NOT include them in the article.\n",
    "- Do not include any disclaimers, model metadata, license text, or statements like 'This article was generated'. Output only the article text.\n",
    "\n",
    "[Start the article below]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize input\n",
    "input_data = tokenizer(prompt,\n",
    "                       return_tensors=\"pt\",\n",
    "                       truncation=True).to(model.device)\n",
    "\n",
    "# Set PAD token_id\n",
    "pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "\n",
    "# Generate article\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_data[\"input_ids\"],\n",
    "    attention_mask=input_data[\"attention_mask\"],\n",
    "    # min_new_tokens = int(len(human_article_tokens_list) * 1.02),\n",
    "    # max_new_tokens = int(len(human_article_tokens_list) * 1.20)\n",
    "    max_new_tokens= int(len(human_article_words_list) * 1.45) + 70,\n",
    "    do_sample=False,\n",
    "    # no_repeat_ngram_size=3,\n",
    "    # repetition_penalty=1.05,\n",
    "    pad_token_id=pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "rag_version = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove the prompt and the end of article placeholders from the LLM response\n",
    "for unwanted_string in [prompt, '[End of article]', '<END_ARTICLE>', '<END>']:\n",
    "  rag_version = rag_version.replace(unwanted_string, \"\")\n",
    "\n",
    "print(\"RAG-ENHANCED LLM ARTICLE -- \")\n",
    "print(wrapper(rag_version))\n",
    "words = rag_version.split()\n",
    "print(f\"Word Count: {len(words)}\")\n",
    "tokens = tokenizer.encode(rag_version, truncation=False, add_special_tokens=False)\n",
    "print(f\"Token Count: {len(tokens)}\")\n",
    "\n",
    "# Save\n",
    "filename = f\"{data['title']}_rag_enhanced_article_V3.txt\".replace(\" \", \"_\")\n",
    "with open(f\"/content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(rag_version)\n",
    "data['rag_llm_article'] = rag_version\n",
    "print(f\"RAG-enhanced article saved to /content/drive/MyDrive/Colab Notebooks/Dissertation/Article_{ARTICLE_INDEX}/{filename}\")\n",
    "\n",
    "main_data[ARTICLE_INDEX] = data\n",
    "json.dump(main_data, open(\"wikinews_data.json\", \"w\"), indent=3, force_ascii=False, orient=\"records\")"
   ],
   "metadata": {
    "id": "IfEiyGCYF8Mf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1756343797524,
     "user_tz": -60,
     "elapsed": 119297,
     "user": {
      "displayName": "Vishak Lv",
      "userId": "09816053158646633752"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f7de0634-ac9e-4094-d93a-f3f29f9f4268"
   },
   "execution_count": 93,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RAG-ENHANCED LLM ARTICLE -- \n",
      " European Union to reduce carbon emissions by 55% of 1990 levels by 2030  The European Union (EU)\n",
      "has agreed on a new law to cut carbon emissions by at least 55% by 2030, compared with 1990 levels.\n",
      "The decision was reached after 14 hours of intensive negotiations between member states and the EU\n",
      "Parliament.  The new target, which was initially announced by EU leaders in December, is a\n",
      "significant increase from the previous 40% reduction target. The development is seen as a key\n",
      "milestone on the road to the United Nations climate change conference (COP26) in Glasgow this\n",
      "November, where leaders will underscore the need to limit planetary warming to 1.5°C by the end of\n",
      "the century.  Under the new law, the EU will invest further in 'carbon sinks' such as forests,\n",
      "grasslands, and wetlands. MEPs involved in the talks claimed this would in effect equate to a 57%\n",
      "net reduction target for member states. The annual reduction in the 'cap' on emissions from EU\n",
      "Emissions Trading System (ETS) sectors will be increased from 1.74% now to 2.2% after 2020.\n",
      "However, environmental groups have criticized the new target, describing it as a 'farce.' Wendel\n",
      "Trio, the director of Climate Action Network (CAN) Europe, an alliance of environmental NGOs, said\n",
      "the target was not in line with the Paris agreement's ambition to limit temperature rise to 1.5°C by\n",
      "the end of the century.  The UK, meanwhile, announced radical plans to cut carbon emissions by 78%\n",
      "by 2035 earlier this week. However, environmentalists warn that the government has consistently\n",
      "failed to achieve previous targets set by its independent Climate Change Committee (CCC).  The EU\n",
      "Climate Law was agreed in the early hours of Wednesday after months of talks. It sets a limit on the\n",
      "levels of CO2 removal that can count towards the 2030 target, to ensure that states actively lower\n",
      "emissions rather than removing them from the atmosphere through forests, for example. A 15-member\n",
      "independent council will also be established to advise the EU on climate measures and targets.  The\n",
      "project is also detached from the realities of European politics, according to Joanna Flisowska,\n",
      "head of climate and energy at Greenpeace Polska. The EU parliament had pushed for a higher target of\n",
      "a 60% reduction, but MEPs failed to significantly raise the ambitions of the EU's capitals despite\n",
      "the claims on Wednesday that the bloc was showing global leadership on the climate emergency.  The\n",
      "deal will now be fine-tuned by lawyers ahead of formal adoption by the institutions. The EU\n",
      "Commission will announce a package of climate laws in June to support the plans.\n",
      "Word Count: 433\n",
      "Token Count: 632\n",
      "RAG-enhanced article saved to /content/drive/MyDrive/Colab Notebooks/Dissertation/Article_14/European_Union_to_reduce_carbon_emissions_by_55%_of_1990_levels_by_2030_rag_enhanced_article_V3.txt\n"
     ]
    }
   ]
  }
 ]
}
